{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "496e699b",
      "metadata": {
        "id": "496e699b"
      },
      "source": [
        "# End-to-end Vehicles/Pedestrians Segmentation with Labellerr + YOLOv8-seg + ByteTrack\n",
        "\n",
        "This notebook implements an end-to-end, modular workflow:\n",
        "- Data collection (Kaggle dataset)\n",
        "- Annotation using Labellerr SDK (project, upload, export to YOLO-seg)\n",
        "- Training YOLOv8 segmentation (~100 epochs)\n",
        "- Inference + evaluation on held-out test set\n",
        "- Uploading predictions as pre-annotations to a Labellerr test project\n",
        "- Video tracking with ByteTrack and export to results.json\n",
        "- Packaging artifacts and a PDF summary\n",
        "\n",
        "Run cells one-by-one. Cells are designed to be safe on Colab too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "d99576ac",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d99576ac",
        "outputId": "68b06d58-edaa-48ac-b906-478f7d498485"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m45.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.0 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.9/2.0 MB\u001b[0m \u001b[31m82.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/207.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.2/207.2 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for labellerr_sdk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Base dir: /content\n",
            "Dirs ready: data/images/raw data/labels artifacts apps\n"
          ]
        }
      ],
      "source": [
        "# Section 1: Environment Setup and Project Structure\n",
        "import os, sys, shutil, json, random\n",
        "from pathlib import Path\n",
        "\n",
        "# Colab-friendly dependency installs\n",
        "try:\n",
        "    import google.colab  # type: ignore\n",
        "    IN_COLAB = True\n",
        "except Exception:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Core libs and training/eval stack\n",
        "    !pip -q install ultralytics opencv-python-headless seaborn matplotlib pandas tqdm reportlab pycocotools supervision\n",
        "    # Labellerr SDK (optional; only needed if using their API)\n",
        "    !pip -q install https://github.com/tensormatics/SDKPython/releases/download/prod/labellerr_sdk-1.0.0.tar.gz\n",
        "    # KaggleHub for dataset download\n",
        "    !pip -q install kagglehub\n",
        "\n",
        "BASE_DIR = Path('.')\n",
        "IMAGES_RAW_DIR = BASE_DIR / 'data' / 'images' / 'raw'\n",
        "LABELS_DIR = BASE_DIR / 'data' / 'labels'\n",
        "ARTIFACTS_DIR = BASE_DIR / 'artifacts'\n",
        "APPS_DIR = BASE_DIR / 'apps'\n",
        "for p in [IMAGES_RAW_DIR, LABELS_DIR, ARTIFACTS_DIR, APPS_DIR]:\n",
        "    p.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print('Base dir:', BASE_DIR.resolve())\n",
        "print('Dirs ready:', IMAGES_RAW_DIR, LABELS_DIR, ARTIFACTS_DIR, APPS_DIR)\n",
        "\n",
        "def ensure_dirs(paths):\n",
        "    for p in paths:\n",
        "        Path(p).mkdir(parents=True, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "90529ae1",
      "metadata": {
        "id": "90529ae1"
      },
      "source": [
        "## Section 2: Dataset Download (Kaggle)\n",
        "We will use the Kaggle dataset \"ashfakyeafi/road-vehicle-images-dataset\" via KaggleHub and stage the images into `data/images/raw` for this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1486c902",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1486c902",
        "outputId": "cde6259c-67f1-45e3-8f88-2eb2ce0b99a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/ashfakyeafi/road-vehicle-images-dataset?dataset_version_number=2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 115M/115M [00:00<00:00, 182MB/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/ashfakyeafi/road-vehicle-images-dataset/versions/2\n",
            "Copied 3004 new images. Total in data/images/raw: 3004\n"
          ]
        }
      ],
      "source": [
        "# Dataset download and staging (Kaggle)\n",
        "import os, shutil, glob\n",
        "from pathlib import Path\n",
        "\n",
        "try:\n",
        "    import kagglehub as kh\n",
        "except Exception:\n",
        "    # If running locally, ensure kagglehub is installed. In Colab, Section 1 installs it.\n",
        "    !pip -q install kagglehub\n",
        "    import kagglehub as kh\n",
        "\n",
        "# OPTIONAL: Kaggle auth only if dataset requires it\n",
        "# from google.colab import files\n",
        "# files.upload()  # upload kaggle.json\n",
        "# Path('~/.kaggle').expanduser().mkdir(parents=True, exist_ok=True)\n",
        "# shutil.move('kaggle.json', str(Path('~/.kaggle/kaggle.json').expanduser()))\n",
        "# os.chmod(str(Path('~/.kaggle/kaggle.json').expanduser()), 0o600)\n",
        "\n",
        "# Download dataset\n",
        "ds_path = kh.dataset_download(\"ashfakyeafi/road-vehicle-images-dataset\")\n",
        "print(\"Path to dataset files:\", ds_path)\n",
        "\n",
        "# Stage images into the notebook's raw images folder\n",
        "IMAGES_RAW_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "# Basic image validation\n",
        "import cv2\n",
        "\n",
        "def is_valid_image(path: str) -> bool:\n",
        "    try:\n",
        "        img = cv2.imread(path)\n",
        "        return img is not None\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "# Collect common image types from the downloaded dataset\n",
        "img_exts = (\"*.jpg\", \"*.jpeg\", \"*.png\", \"*.bmp\")\n",
        "found = []\n",
        "for pat in img_exts:\n",
        "    found.extend(glob.glob(str(Path(ds_path) / \"**\" / pat), recursive=True))\n",
        "\n",
        "# Copy (skip existing) into data/images/raw\n",
        "copied = 0\n",
        "existing_names = {p.name for p in IMAGES_RAW_DIR.glob(\"*\")}\n",
        "for src in found:\n",
        "    src_p = Path(src)\n",
        "    if src_p.name in existing_names:\n",
        "        continue\n",
        "    if is_valid_image(str(src_p)):\n",
        "        shutil.copy2(src_p, IMAGES_RAW_DIR / src_p.name)\n",
        "        copied += 1\n",
        "\n",
        "total = len(list(IMAGES_RAW_DIR.glob(\"*\")))\n",
        "print(f\"Copied {copied} new images. Total in {IMAGES_RAW_DIR}: {total}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3ea635cb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ea635cb",
        "outputId": "7e5ad311-62a2-4bb0-bafb-63805a146c54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "sources.md at data/sources.md\n"
          ]
        }
      ],
      "source": [
        "# Build sources.md from images present in data/images/raw\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "def build_sources_md_from_dir(images_dir: Path, out_md: str = 'data/sources.md'):\n",
        "    images = sorted([str(p) for p in Path(images_dir).glob('*.jpg')])\n",
        "    lines = [\"# Image Sources and Licenses\\n\"]\n",
        "    for p in images:\n",
        "        # Source unknown because of Kaggle packaging; update if you have per-file metadata\n",
        "        lines.append(f\"- file: {p}, url: unknown (Kaggle package), license: check dataset license\")\n",
        "    Path(out_md).parent.mkdir(parents=True, exist_ok=True)\n",
        "    Path(out_md).write_text(\"\\n\".join(lines), encoding='utf-8')\n",
        "    return out_md\n",
        "\n",
        "sources_md_path = build_sources_md_from_dir(IMAGES_RAW_DIR)\n",
        "print('sources.md at', sources_md_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a7234ec",
      "metadata": {
        "id": "2a7234ec"
      },
      "source": [
        "## Section 3: Labellerr SDK Auth and Project Creation\n",
        "Initialize the Labellerr SDK client and create a training project with an ontology for person and vehicle polygon masks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c072ca11",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c072ca11",
        "outputId": "0028f050-05c2-49e9-d04a-f24fc3aac2c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project creation error: LabellerrClient.create_project() missing 5 required positional arguments: 'data_type', 'client_id', 'dataset_id', 'annotation_template_id', and 'rotation_config'\n",
            "Train project id: None\n"
          ]
        }
      ],
      "source": [
        "# Labellerr client and project creation helpers\n",
        "import os\n",
        "\n",
        "# Install in Colab if needed:\n",
        "# !pip -q install https://github.com/tensormatics/SDKPython/releases/download/prod/labellerr_sdk-1.0.0.tar.gz\n",
        "\n",
        "try:\n",
        "    from labellerr.client import LabellerrClient  # type: ignore\n",
        "except Exception as e:\n",
        "    LabellerrClient = None\n",
        "    print('Labellerr SDK not installed. Install from docs if needed.')\n",
        "\n",
        "\n",
        "def labellerr_client():\n",
        "    if LabellerrClient is None:\n",
        "        return None\n",
        "    api_key = os.getenv('LABELLERR_API_KEY','8d24e9.bc324a450a90c026b763a2ae8a')\n",
        "    api_secret = os.getenv('LABELLERR_API_SECRET','5b7dda285f37211090a4ac04e1c3c0dfdf7152e110070ad487947a159ce6aff1')\n",
        "    if not api_key or not api_secret:\n",
        "        print('Set LABELLERR_API_KEY and LABELLERR_API_SECRET in env')\n",
        "        return None\n",
        "    return LabellerrClient(api_key, api_secret)\n",
        "\n",
        "\n",
        "def labellerr_create_project(client, name='Vehicles-Pedestrians-SEG', desc='Training segmentation project', ontology=None):\n",
        "    if client is None:\n",
        "        return None\n",
        "    # Placeholder; exact SDK calls depend on Labellerr SDK\n",
        "    try:\n",
        "        # Example skeleton (adjust per SDK):\n",
        "        project = client.create_project({\n",
        "            'name': name,\n",
        "            'description': desc,\n",
        "            'ontology': ontology or {\n",
        "                'tools': ['polygon'],\n",
        "                'classes': ['person', 'vehicle']\n",
        "            },\n",
        "            'data_type': 'image', # Replace with your data type\n",
        "            'client_id': 'your_client_id', # Replace with your client ID\n",
        "            'dataset_id': 'your_dataset_id', # Replace with your dataset ID\n",
        "            'annotation_template_id': 'your_template_id', # Replace with your template ID\n",
        "            'rotation_config': {} # Replace with your rotation config\n",
        "        })\n",
        "        print('Created project:', project)\n",
        "        return project.get('id') if isinstance(project, dict) else project\n",
        "    except Exception as e:\n",
        "        print('Project creation error:', e)\n",
        "        return None\n",
        "\n",
        "client = labellerr_client()\n",
        "# Example call:\n",
        "train_project_id = labellerr_create_project(client)\n",
        "print('Train project id:', train_project_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8cf7e248",
      "metadata": {
        "id": "8cf7e248"
      },
      "source": [
        "## Section 5: Upload Images and Create Polygon Segmentation Task\n",
        "Upload up to 100 images and create an annotation task. This section includes polling until completion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "e3e74699",
      "metadata": {
        "id": "e3e74699"
      },
      "outputs": [],
      "source": [
        "# Upload images and create annotation task (SDK placeholders)\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "def labellerr_upload_images(client, project_id, image_dir, limit=100):\n",
        "    if client is None or not project_id:\n",
        "        print('Client or project missing')\n",
        "        return []\n",
        "    imgs = sorted([str(p) for p in Path(image_dir).glob('*.jpg')])[:limit]\n",
        "    uploaded_ids = []\n",
        "    for p in imgs:\n",
        "        try:\n",
        "            # Placeholder for actual SDK upload\n",
        "            r = client.upload_image(project_id=project_id, image_path=p)\n",
        "            uploaded_ids.append(r.get('id') if isinstance(r, dict) else r)\n",
        "        except Exception as e:\n",
        "            print('Upload failed for', p, e)\n",
        "    print('Uploaded', len(uploaded_ids), 'images')\n",
        "    return uploaded_ids\n",
        "\n",
        "\n",
        "def labellerr_create_annotation_task(client, project_id, assignees=None, instructions='Annotate vehicles and pedestrians with polygon masks'):\n",
        "    if client is None or not project_id:\n",
        "        return None\n",
        "    try:\n",
        "        task = client.create_task({\n",
        "            'project_id': project_id,\n",
        "            'type': 'segmentation',\n",
        "            'tool': 'polygon',\n",
        "            'instructions': instructions,\n",
        "            'assignees': assignees or [],\n",
        "        })\n",
        "        print('Created task:', task)\n",
        "        return task\n",
        "    except Exception as e:\n",
        "        print('Create task error:', e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def poll_task_until_complete(client, task_id, timeout=7200, interval=20):\n",
        "    start = time.time()\n",
        "    while time.time() - start < timeout:\n",
        "        try:\n",
        "            status = client.get_task_status(task_id)\n",
        "            print('Task status:', status)\n",
        "            if status in ['completed','done','exported']:\n",
        "                return status\n",
        "        except Exception as e:\n",
        "            print('Poll error:', e)\n",
        "        time.sleep(interval)\n",
        "    return 'timeout'\n",
        "\n",
        "# Example calls (commented to avoid accidental execution without credentials)\n",
        "uploaded = labellerr_upload_images(client, train_project_id, IMAGES_RAW_DIR)\n",
        "# task = labellerr_create_annotation_task(client, train_project_id)\n",
        "# poll_task_until_complete(client, task.get('id'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e41b6f99",
      "metadata": {
        "id": "e41b6f99"
      },
      "source": [
        "## Section 6: Export Annotations from Labellerr in YOLO-Seg Format\n",
        "Export labels in YOLO segmentation format and validate pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "37d7bcb0",
      "metadata": {
        "id": "37d7bcb0"
      },
      "outputs": [],
      "source": [
        "# Export and validate YOLO-seg labels (SDK placeholders)\n",
        "import zipfile\n",
        "\n",
        "def labellerr_export_yolo_seg(client, project_id, out_dir='data/labels'):\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    if client is None or not project_id:\n",
        "        print('Client/project missing')\n",
        "        return None\n",
        "    try:\n",
        "        # Placeholder for actual export call\n",
        "        zpath = client.export_annotations(project_id=project_id, format='yolo-seg')  # returns zip path or URL\n",
        "        # If URL, download to out_dir; here assume local path\n",
        "        dst = Path(out_dir) / 'labels_export.zip'\n",
        "        shutil.copy(zpath, dst)\n",
        "        with zipfile.ZipFile(dst, 'r') as zf:\n",
        "            zf.extractall(out_dir)\n",
        "        print('Exported to', out_dir)\n",
        "        return out_dir\n",
        "    except Exception as e:\n",
        "        print('Export error:', e)\n",
        "        return None\n",
        "\n",
        "\n",
        "def validate_pairs(images_dir, labels_dir):\n",
        "    images = {p.stem for p in Path(images_dir).glob('*.jpg')}\n",
        "    labels = {p.stem for p in Path(labels_dir).glob('*.txt')}\n",
        "    missing = images - labels\n",
        "    if missing:\n",
        "        print('Missing labels for images:', list(sorted(missing))[:10], '...')\n",
        "    else:\n",
        "        print('All image-label pairs present.')\n",
        "\n",
        "# Example\n",
        "# export_dir = labellerr_export_yolo_seg(client, train_project_id, out_dir=str(LABELS_DIR))\n",
        "# validate_pairs(IMAGES_RAW_DIR, LABELS_DIR)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b5f640d",
      "metadata": {
        "id": "7b5f640d"
      },
      "source": [
        "## Section 7: Prepare YOLO Data Folders and YAML Splits\n",
        "Create train/val/test splits and generate a `data.yaml` for YOLOv8-seg."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "ca73d21d",
      "metadata": {
        "id": "ca73d21d"
      },
      "outputs": [],
      "source": [
        "# Splitting and data.yaml helpers\n",
        "import random, yaml\n",
        "\n",
        "\n",
        "def make_splits(images_dir, labels_dir, out_base='data', ratios=(0.7,0.2,0.1), seed=42):\n",
        "    images = sorted([p for p in Path(images_dir).glob('*.jpg')])\n",
        "    random.Random(seed).shuffle(images)\n",
        "    n = len(images)\n",
        "    n_tr = int(ratios[0]*n)\n",
        "    n_va = int(ratios[1]*n)\n",
        "    parts = {\n",
        "        'train': images[:n_tr],\n",
        "        'val': images[n_tr:n_tr+n_va],\n",
        "        'test': images[n_tr+n_va:]\n",
        "    }\n",
        "    for split, im_list in parts.items():\n",
        "        img_out = Path(out_base)/'images'/split\n",
        "        lbl_out = Path(out_base)/'labels'/split\n",
        "        img_out.mkdir(parents=True, exist_ok=True)\n",
        "        lbl_out.mkdir(parents=True, exist_ok=True)\n",
        "        for ip in im_list:\n",
        "            lp = Path(labels_dir)/ (ip.stem + '.txt')\n",
        "            if lp.exists():\n",
        "                shutil.copy(ip, img_out/ip.name)\n",
        "                shutil.copy(lp, lbl_out/lp.name)\n",
        "    return parts\n",
        "\n",
        "\n",
        "def write_data_yaml(path='data/data.yaml', names=('person','vehicle')):\n",
        "    data = {\n",
        "        'path': 'data',\n",
        "        'train': 'images/train',\n",
        "        'val': 'images/val',\n",
        "        'test': 'images/test',\n",
        "        'names': list(names),\n",
        "        'nc': len(names)\n",
        "    }\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(path, 'w') as f:\n",
        "        yaml.safe_dump(data, f)\n",
        "    return path\n",
        "\n",
        "# Example once labels are present:\n",
        "# parts = make_splits(IMAGES_RAW_DIR, LABELS_DIR)\n",
        "# data_yaml = write_data_yaml()\n",
        "# print('Data YAML at', data_yaml)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b430a52c",
      "metadata": {
        "id": "b430a52c"
      },
      "source": [
        "## Section 8: Train YOLOv8-seg (~100 epochs) and Save Metrics\n",
        "Train using Ultralytics and collect metrics and plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a640679f",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a640679f",
        "outputId": "0739bfab-8ee3-4799-954e-e4715d6fa47a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "# Training helper\n",
        "from ultralytics import YOLO\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def train_yolov8_seg(model='yolov8n-seg.pt', data='data/data.yaml', epochs=100, imgsz=640, project='artifacts/runs', name='train-seg'):\n",
        "    Path(project).mkdir(parents=True, exist_ok=True)\n",
        "    m = YOLO(model)\n",
        "    r = m.train(data=data, epochs=epochs, imgsz=imgsz, project=project, name=name, save=True, plots=True)\n",
        "    run_dir = Path(r.save_dir)\n",
        "    # Copy key plots\n",
        "    metrics_dir = Path('artifacts/metrics')\n",
        "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "    for plot_name in ['F1_curve.png','PR_curve.png','R_curve.png','P_curve.png','confusion_matrix.png']:\n",
        "        src = run_dir/plot_name\n",
        "        if src.exists():\n",
        "            shutil.copy(src, metrics_dir/src.name)\n",
        "    # Results CSV\n",
        "    results_csv = run_dir/'results.csv'\n",
        "    if results_csv.exists():\n",
        "        df = pd.read_csv(results_csv)\n",
        "        df.to_csv(metrics_dir/'results.csv', index=False)\n",
        "    return str(run_dir)\n",
        "\n",
        "# Example:\n",
        "run_dir = train_yolov8_seg(epochs=100)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "695b528e",
      "metadata": {
        "id": "695b528e"
      },
      "source": [
        "## Section 9: Evaluate on Test Set and Plot Metrics\n",
        "Validate the trained model and save metrics/plots."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2509fab3",
      "metadata": {
        "id": "2509fab3"
      },
      "outputs": [],
      "source": [
        "# Evaluation helper\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def evaluate_model(weights, data_yaml='data/data.yaml', split='test'):\n",
        "    m = YOLO(weights)\n",
        "    r = m.val(data=data_yaml, split=split, save_json=True, plots=True)\n",
        "    out_dir = Path(r.save_dir)\n",
        "    metrics_dir = Path('artifacts/metrics')\n",
        "    metrics_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # Copy plots/metrics\n",
        "    for p in out_dir.glob('*.png'):\n",
        "        shutil.copy(p, metrics_dir/p.name)\n",
        "    for p in out_dir.glob('*.json'):\n",
        "        shutil.copy(p, metrics_dir/p.name)\n",
        "    print('Metrics saved to artifacts/metrics')\n",
        "    return str(out_dir)\n",
        "\n",
        "# Example:\n",
        "# eval_dir = evaluate_model(weights='artifacts/runs/train-seg/weights/best.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc8802d2",
      "metadata": {
        "id": "bc8802d2"
      },
      "source": [
        "## Section 10: Inference on Unseen Images and Save predictions.json\n",
        "Run inference and save predictions (boxes, classes, masks as polygons if available)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "8da3bf9e",
      "metadata": {
        "id": "8da3bf9e"
      },
      "outputs": [],
      "source": [
        "# Inference helper\n",
        "from shapely.geometry import Polygon\n",
        "\n",
        "\n",
        "def masks_to_polygons(masks):\n",
        "    polys = []\n",
        "    if masks is None:\n",
        "        return polys\n",
        "    try:\n",
        "        for m in masks.xy:  # Ultralytics provides polygons for seg models\n",
        "            # m is list of [x,y] points\n",
        "            pts = [float(v) for v in m.reshape(-1).tolist()]\n",
        "            polys.append(pts)\n",
        "    except Exception:\n",
        "        pass\n",
        "    return polys\n",
        "\n",
        "\n",
        "def infer_and_save_json(weights, source='data/images/test', out_json='artifacts/predictions.json', conf=0.25, iou=0.7, imgsz=640):\n",
        "    m = YOLO(weights)\n",
        "    results = m(source=source, conf=conf, iou=iou, imgsz=imgsz, save=False, stream=True, verbose=False)\n",
        "    preds = []\n",
        "    for r in results:\n",
        "        im_path = getattr(r, 'path', None)\n",
        "        names = r.names\n",
        "        if r.boxes is not None:\n",
        "            boxes = r.boxes.xyxy.cpu().numpy().tolist()\n",
        "            cls = r.boxes.cls.int().cpu().numpy().tolist()\n",
        "            confs = r.boxes.conf.cpu().numpy().tolist()\n",
        "            polys = masks_to_polygons(getattr(r, 'masks', None))\n",
        "            for i, (b, c, s) in enumerate(zip(boxes, cls, confs)):\n",
        "                item = {\n",
        "                    'image_path': im_path,\n",
        "                    'cls': int(c),\n",
        "                    'cls_name': names.get(int(c), str(c)) if isinstance(names, dict) else str(c),\n",
        "                    'conf': float(s),\n",
        "                    'bbox_xyxy': [float(v) for v in b],\n",
        "                }\n",
        "                if i < len(polys):\n",
        "                    item['mask_polygon'] = polys[i]\n",
        "                preds.append(item)\n",
        "    Path(out_json).parent.mkdir(parents=True, exist_ok=True)\n",
        "    with open(out_json, 'w') as f:\n",
        "        json.dump(preds, f, indent=2)\n",
        "    print('Saved predictions to', out_json)\n",
        "    return out_json\n",
        "\n",
        "# Example:\n",
        "# pred_json = infer_and_save_json('artifacts/runs/train-seg/weights/best.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edb15e99",
      "metadata": {
        "id": "edb15e99"
      },
      "source": [
        "## Section 11: Labellerr Test Project and Pre-annotations Upload\n",
        "Create a test project and upload predictions as pre-annotations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "d9cd00d1",
      "metadata": {
        "id": "d9cd00d1"
      },
      "outputs": [],
      "source": [
        "# Pre-annotation upload (SDK placeholders)\n",
        "\n",
        "def labellerr_upload_preannotations(client, project_id, predictions_json):\n",
        "    if client is None or not project_id:\n",
        "        print('Client/project missing')\n",
        "        return False\n",
        "    data = json.load(open(predictions_json,'r'))\n",
        "    # Convert to expected payload; structure depends on SDK specifics.\n",
        "    payload = []\n",
        "    for item in data:\n",
        "        payload.append({\n",
        "            'image_path': item['image_path'],\n",
        "            'annotations': [{\n",
        "                'class': item['cls_name'],\n",
        "                'bbox_xyxy': item['bbox_xyxy'],\n",
        "                'polygon': item.get('mask_polygon')\n",
        "            }]\n",
        "        })\n",
        "    try:\n",
        "        resp = client.upload_preannotations(project_id=project_id, annotations=payload)\n",
        "        print('Pre-annotations upload response:', resp)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print('Pre-annotations upload error:', e)\n",
        "        return False\n",
        "\n",
        "# Example:\n",
        "# test_project_id = labellerr_create_project(client, name='Vehicles-Pedestrians-SEG-Test', desc='Test project')\n",
        "# ok = labellerr_upload_preannotations(client, test_project_id, 'artifacts/predictions.json')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "27513580",
      "metadata": {
        "id": "27513580"
      },
      "source": [
        "## Section 12: Video Tracking with YOLOv8 + ByteTrack\n",
        "Use Ultralytics' built-in ByteTrack integration to track vehicles and pedestrians in videos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "667cfe32",
      "metadata": {
        "id": "667cfe32"
      },
      "outputs": [],
      "source": [
        "# Tracking helper using Ultralytics' ByteTrack\n",
        "\n",
        "def track_video(weights, video_path, out_dir='artifacts/tracking', tracker='bytetrack.yaml', imgsz=640, conf=0.25):\n",
        "    from ultralytics import YOLO\n",
        "    Path(out_dir).mkdir(parents=True, exist_ok=True)\n",
        "    m = YOLO(weights)\n",
        "    res = m.track(source=video_path, tracker=tracker, imgsz=imgz, conf=conf, save=True, persist=True, project=out_dir, name='track_run')\n",
        "    # Build results.json\n",
        "    results_json = []\n",
        "    for r in res:\n",
        "        if r.boxes is None or r.boxes.id is None:\n",
        "            continue\n",
        "        ids = r.boxes.id.int().cpu().tolist()\n",
        "        xyxy = r.boxes.xyxy.cpu().numpy().tolist()\n",
        "        cls = r.boxes.cls.int().cpu().tolist()\n",
        "        names = r.names\n",
        "        frame_idx = getattr(r, 'frame', None)\n",
        "        for tid, box, c in zip(ids, xyxy, cls):\n",
        "            results_json.append({'frame': int(frame_idx) if frame_idx is not None else -1,\n",
        "                                 'track_id': int(tid), 'class_id': int(c), 'class_name': names.get(int(c), str(c)) if isinstance(names, dict) else str(c),\n",
        "                                 'bbox_xyxy': [float(v) for v in box]})\n",
        "    out_json = Path(out_dir)/'results.json'\n",
        "    with open(out_json, 'w') as f:\n",
        "        json.dump(results_json, f, indent=2)\n",
        "    print('Tracking results saved to', out_json)\n",
        "    return str(out_json)\n",
        "\n",
        "# Example:\n",
        "# results_json = track_video(weights='yolov8n.pt', video_path='sample.mp4')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c24aad83",
      "metadata": {
        "id": "c24aad83"
      },
      "source": [
        "## Section 13: Streamlit App Wrapper\n",
        "Use the provided app at `apps/tracker_app.py` and run it locally to test tracking UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f49072c5",
      "metadata": {
        "id": "f49072c5"
      },
      "outputs": [],
      "source": [
        "# To run locally (PowerShell):\n",
        "# python -m venv .venv; .\\.venv\\Scripts\\Activate.ps1\n",
        "# pip install -r requirements.txt\n",
        "# streamlit run apps/tracker_app.py\n",
        "print('Tracker app path:', (BASE_DIR/'apps'/'tracker_app.py').resolve())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7406aa0",
      "metadata": {
        "id": "b7406aa0"
      },
      "source": [
        "## Section 14: Artifact Packaging (README and PDF Summary)\n",
        "Generate a README and a PDF summary report from metrics and examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ca1399b",
      "metadata": {
        "id": "8ca1399b"
      },
      "outputs": [],
      "source": [
        "# README and PDF summary\n",
        "from reportlab.pdfgen import canvas\n",
        "from reportlab.lib.pagesizes import A4\n",
        "\n",
        "def write_readme(path='artifacts/README.md', context=None):\n",
        "    lines = [\n",
        "        '# Results and Notes',\n",
        "        '',\n",
        "        'This folder contains metrics, plots, and example outputs.',\n",
        "        'Refer to root README.md for setup and instructions.',\n",
        "    ]\n",
        "    if context:\n",
        "        lines += ['', '## Context', json.dumps(context, indent=2)]\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    Path(path).write_text('\\n'.join(lines), encoding='utf-8')\n",
        "    return path\n",
        "\n",
        "\n",
        "def export_pdf_summary(out_pdf='artifacts/summary.pdf', metrics_dir='artifacts/metrics'):\n",
        "    Path(out_pdf).parent.mkdir(parents=True, exist_ok=True)\n",
        "    c = canvas.Canvas(out_pdf, pagesize=A4)\n",
        "    w, h = A4\n",
        "    y = h - 50\n",
        "    c.setFont('Helvetica-Bold', 16)\n",
        "    c.drawString(50, y, 'Labellerr + YOLOv8-seg + ByteTrack Summary')\n",
        "    y -= 30\n",
        "    c.setFont('Helvetica', 12)\n",
        "    c.drawString(50, y, 'Key metrics and plots')\n",
        "    y -= 20\n",
        "    # Embed a couple of plots if present\n",
        "    for img_name in ['PR_curve.png','confusion_matrix.png','F1_curve.png']:\n",
        "        p = Path(metrics_dir)/img_name\n",
        "        if p.exists():\n",
        "            y -= 300\n",
        "            c.drawImage(str(p), 50, y, width=500, height=280, preserveAspectRatio=True, anchor='nw')\n",
        "            y -= 20\n",
        "            c.drawString(50, y, img_name)\n",
        "            y -= 10\n",
        "    c.showPage()\n",
        "    c.save()\n",
        "    print('PDF summary at', out_pdf)\n",
        "    return out_pdf\n",
        "\n",
        "# Example:\n",
        "# readme_path = write_readme()\n",
        "# pdf_path = export_pdf_summary()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
