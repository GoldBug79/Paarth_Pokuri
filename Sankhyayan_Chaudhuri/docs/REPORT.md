# Labellerr + YOLOv8-Seg + ByteTrack: Journey, Guide, and Lessons

This report documents the end-to-end process to build an object segmentation and tracking workflow for vehicles and pedestrians using Labellerr for annotation, YOLOv8 for training and inference, and ByteTrack for multi-object tracking. It includes our journey, issues encountered and fixes, and a practical guide to help you build your own tracker.

## 1. Overview
- Objective: Train an instance/semantic segmentation model for vehicles/pedestrians and run multi-object tracking on videos, producing `results.json`.
- Stack: Ultralytics YOLOv8-seg, Labellerr SDK, ByteTrack (via Ultralytics), Streamlit app for local tracking, Colab support for training.
- Artifacts: `artifacts/metrics`, `artifacts/summary.pdf`, `artifacts/tracking/results.json`.

## 2. Project Structure
- `notebooks/labellerr_yolov8_seg_workflow.ipynb`: Main E2E workflow (Colab-friendly)
- `apps/tracker_app.py`: Minimal Streamlit tracker app
- `app/streamlit_app.py`: Richer local tracker app with memory controls
- `src/utils.py`: Utilities for data and Labellerr helpers
- `data/`: Images and labels
- `artifacts/`: Training runs, metrics, predictions, reports

## 3. Journey & Issues Faced
- Data Sourcing:
  - Initial approach used Open Images; replaced with a Kaggle dataset to make Colab usage simpler and reproducible.
  - Fix: Added KaggleHub integration and removed duplicate downloading, ensuring unique filenames and skip-when-present behavior.
- Memory Constraints in Tracking:
  - Issue: RAM spikes when saving annotated videos and accumulating per-frame results in memory.
  - Fix: Streamed JSON writing, optional video saving, frame stride, max frames cap, reduced `imgsz`, and periodic GC.
- Labellerr SDK Alignment:
  - The SDK’s exact methods vary by version; we used placeholders with clear entry points. The notebook guides setting `LABELLERR_API_KEY` and `LABELLERR_API_SECRET` and provides helper stubs (`create_project`, `upload_image`, `export_annotations`, `upload_preannotations`). Adjust to your installed SDK’s function names.
- Environment Setup & Colab:
  - Added automatic pip installs in the notebook for a one-click Colab run.
- Reproducibility:
  - Notebook generates `sources.md`, consolidates metrics into `artifacts/metrics`, and a PDF summary into `artifacts/summary.pdf`.

## 4. Step-by-Step: Build Your Own Tracker with Labellerr
1) Prepare Environment (Colab or local)
- Colab: Open the notebook and run the first cell. It installs dependencies and creates folders.
- Local: Use a Python 3.10+ venv and `pip install -r requirements.txt`.

2) Data via Kaggle
- The notebook downloads `ashfakyeafi/road-vehicle-images-dataset` with KaggleHub and stages images into `data/images/raw`.
- If your dataset requires authentication, upload `kaggle.json` and set permissions as shown in the notebook.

3) Annotate with Labellerr
- Create a Labellerr project with classes (e.g., `person`, `vehicle`) and polygon tool.
- Upload up to 100 images (or more as needed) using the SDK helper. Poll task until completed.
- Export annotations in YOLO-seg format and validate pairs (image/label).

4) Train YOLOv8-seg
- Use the notebook’s `make_splits` to create train/val/test and `write_data_yaml` to generate `data/data.yaml`.
- Train with `train_yolov8_seg` (default: 100 epochs, imgsz=640). Metrics and plots will be copied to `artifacts/metrics`.

5) Evaluate & Inference
- Run `evaluate_model` for test metrics; outputs JSON and plots into `artifacts/metrics`.
- Run `infer_and_save_json` to create `artifacts/predictions.json` with boxes/classes and optional polygons.

6) Pre-annotations Back to Labellerr (Optional)
- Use `labellerr_upload_preannotations` to push model predictions for QA/triage.

7) Video Tracking with ByteTrack
- Notebook helper `track_video` (or Streamlit app) runs tracking and exports `artifacts/tracking/results.json`. Use `imgsz`, `vid_stride`, and `max_frames` for performance.

## 5. Tips & Best Practices
- Hardware: Start with YOLOv8n-seg for baseline, then scale to s/m.
- Label Quality: Consistent ontology and polygon quality dramatically impact segmentation performance.
- Resource Limits: Stream outputs to disk and avoid `save=True` video when RAM is low.
- Version Pinning: Stick to a tested set of library versions if running into incompatibilities.

## 6. Results & Metrics
- The notebook saves: `results.csv`, PR/F1/Recall/Precision curves, confusion matrices, and COCO-style JSONs (if generated by `val`).
- A PDF summary is exported to `artifacts/summary.pdf` with key plots.

## 7. Quick Start Commands (Local)
```powershell
python -m venv .venv
.\.venv\Scripts\Activate.ps1
pip install -r requirements.txt
streamlit run apps\tracker_app.py
```

## 8. Next Steps
- Expand dataset and labels for robustness.
- Add active learning loops: auto-pre-annotate new data and curate in Labellerr.
- Integrate class-filtered tracking (e.g., only vehicles + pedestrians) for your use case.

---
If you run into SDK method mismatches, share the SDK version and I can adapt the helper calls in the notebook accordingly.